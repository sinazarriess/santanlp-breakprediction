% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{howard2013some,
  title={Some improvements on deep convolutional neural network based image classification},
  author={Howard, Andrew G},
  journal={arXiv preprint arXiv:1312.5402},
  year={2013}
}


@article{10.1093/llc/fqv067,
    author = {Finlayson, Mark A.},
    title = "{ProppLearner: Deeply annotating a corpus of Russian folktales to enable the machine learning of a Russian formalist theory}",
    journal = {Digital Scholarship in the Humanities},
    volume = {32},
    number = {2},
    pages = {284-300},
    year = {2015},
    month = {12},
    abstract = "{I describe the collection and deep annotation of the semantics of a corpus of Russian folktales. This corpus, which I call the ‘ProppLearner’ corpus, was assembled to provide data for an algorithm designed to learn Vladimir Propp’s morphology of Russian hero tales. The corpus is the most deeply annotated narrative corpus available at this time. The algorithm and learning results are described elsewhere; here, I provide detail on the layers of annotation and how they were chosen, novel layers of annotation required for successful learning, the selection of the texts for annotation, the annotation process itself, and the resulting inter-annotator agreement measures. In particular, the corpus comprised fifteen texts totaling 18,862 words. There were eighteen layers of annotation, five of which were developed specifically to support learning Propp’s morphology: referent attributes, context relationships, event valences, Propp’s ‘dramatis personae’, and Propp’s functions. All annotations were created by trained annotators with the Story Workbench annotation tool, following a double-annotation paradigm. I discuss lessons learned from this effort and what they mean for future digital humanities efforts when working with the semantics of natural language text.}",
    issn = {2055-7671},
    doi = {10.1093/llc/fqv067},
    url = {https://doi.org/10.1093/llc/fqv067},
    eprint = {https://academic.oup.com/dsh/article-pdf/32/2/284/17655882/fqv067.pdf},
}





@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}


@article{Gius:2021tj,
	author = {Gius, Evelyn and Willand, Marcus and Reiter, Nils},
	date = {2021-12-15},
	day = 15,
	doi = {10.22148/001c.30697},
	journal = {Journal of Cultural Analytics},
	month = 12,
	number = 4,
	publisher = {Department of Languages, Literatures, and Cultures},
	title = {On Organizing a Shared Task for the Digital Humanities -- Conclusions and Future Paths},
	volume = 6,
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.22148/001c.30697}}


@inproceedings{liu-etal-2020-data,
    title = "Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation",
    author = "Liu, Ruibo  and
      Xu, Guangxuan  and
      Jia, Chenyan  and
      Ma, Weicheng  and
      Wang, Lili  and
      Vosoughi, Soroush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.726",
    doi = "10.18653/v1/2020.emnlp-main.726",
    pages = "9031--9041",
    abstract = "Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7{\%} on average when given only 10{\%} of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.",
}

@inproceedings{andreas2020good,
  title={Good-enough compositional data augmentation},
  author={Andreas, Jacob},
  booktitle = "Proceedings of ACL",
  year={2020}
}

@inproceedings{jia-liang-2016-data,
    title = "Data Recombination for Neural Semantic Parsing",
    author = "Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1002",
    doi = "10.18653/v1/P16-1002",
    pages = "12--22",
}

@inproceedings{kumar-etal-2020-data,
    title = "Data Augmentation using Pre-trained Transformer Models",
    author = "Kumar, Varun  and
      Choudhary, Ashutosh  and
      Cho, Eunah",
    booktitle = "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.lifelongnlp-1.3",
    pages = "18--26",
    abstract = "Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.",
}

@inproceedings{wei-zou-2019-eda,
    title = "{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
    author = "Wei, Jason  and
      Zou, Kai",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1670",
    doi = "10.18653/v1/D19-1670",
    pages = "6382--6388",
    abstract = "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50{\%} of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.",
}


@inproceedings{wang-etal-2018-switchout,
    title = "{S}witch{O}ut: an Efficient Data Augmentation Algorithm for Neural Machine Translation",
    author = "Wang, Xinyi  and
      Pham, Hieu  and
      Dai, Zihang  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1100",
    doi = "10.18653/v1/D18-1100",
    pages = "856--861",
    abstract = "In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",
}


@article{belinkov2019,
    author = {Belinkov, Yonatan and Glass, James},
    title = "{Analysis Methods in Neural Language Processing: A Survey}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {7},
    pages = {49-72},
    year = {2019},
    month = {04},
    abstract = "{The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00254},
    url = {https://doi.org/10.1162/tacl\_a\_00254},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00254/1923061/tacl\_a\_00254.pdf},
}



@misc{eltec,
  author = {Lou Burnard and Carolin Odebrecht},
  title = {English Novel Corpus (ELTeC-eng) April 2021 release (v1.0.1)},
  doi = {10.5281/zenodo.4662490},
  year = {2021},
  publisher = {Zenodo}
}

@book{Genette:1980aa,
	address = {Ithaca, New York},
	author = {Genette, G{\'e}rard},
	publisher = {Cornell University Press},
	title = {Narrative Discourse -- An Essay in Method},
	year = {1980}}


@book{Bal:1997aa,
	author = {Bal, Mieke},
	booktitle = {{Narratology: Introduction to the Theory of Narrative}},
	edition = {2},
	publisher = {University of Toronto Press},
	sync = {0},
	title = {{Narratology: Introduction to the Theory of Narrative}},
	year = {1997}}


@inproceedings{piper-etal-2021-narrative,
    title = "Narrative Theory for Computational Narrative Understanding",
    author = "Piper, Andrew  and
      So, Richard Jean  and
      Bamman, David",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.26",
    doi = "10.18653/v1/2021.emnlp-main.26",
    pages = "298--311",
    abstract = "Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it is by and large divorced from the large body of theoretical work on narrative within the humanities, social and cognitive sciences. In this position paper, we introduce the dominant theoretical frameworks to the NLP community, situate current research in NLP within distinct narratological traditions, and argue that linking computational work in NLP to theory opens up a range of new empirical questions that would both help advance our understanding of narrative and open up new practical applications.",
}

@book{ryan_1991,
	address = {Bloomington},
	title = {Possible worlds, artificial intelligence, and narrative theory},
	language = {English},
	publisher = {Indiana University Press},
	author = {Ryan, Marie-Laure},
	year = {1991}
}

@book{fludernik_introduction_2009,
	location = {London ; New York},
	title = {An Introduction to Narratology},
	isbn = {9780415450294 9780415450300 9780203882887},
	pagetotal = {190},
	publisher = {Routledge},
	author = {Fludernik, Monika},
	year = {2009},
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}



@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press}
}

@article{ettinger2020bert,
  title={What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models},
  author={Ettinger, Allyson},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={34--48},
  year={2020},
  publisher={MIT Press}
}



@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@misc{ Gius2019aa,
   Author = { Evelyn Gius and Nils Reiter and Marcus Willand },
   Howpublished = {Special Issue of \textit{Cultural Analytics}},
   Title = {{A Shared Task for the Digital Humanities}},
   Month = nov,
   volume = 2,
   issue = 1,
   Year = { 2019 },
   url = {https://culturalanalytics.org/issue/2254}
}

@article{ Reiter2019ab,
   Title = {{A Shared Task for the Digital Humanities Chapter 1: Introduction to Annotation, Narrative Levels and Shared Tasks}},
   Author = { Nils Reiter and Marcus Willand and Evelyn Gius },
   Issuetitle = {{A Shared Task for the Digital Humanities}},
   Journal = { Cultural Analytics },
   Month = { November },
   Doi = { 10.22148/16.048 },
   Year = { 2019 }
}



@inproceedings{gervas_model_2021,
	location = {Lucca, Tuscany},
	title = {A Model of Interpretation of Embedded Stories},
	url = {http://nil.fdi.ucm.es/sites/default/files/Text2Story2021-paper11-rev.pdf},
	eventtitle = {ext2Story: 4th International Workshop on Narrative Extraction from Texts},
	booktitle = {Proceedings of the Text2Story’21 Workshop},
	publisher = {{CEUR} Workshop Proceedings},
	author = {Gervás, Pablo},
	date = {2021-04-01},
}
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-11-21},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}
