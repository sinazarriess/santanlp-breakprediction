{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/model_doc/bert.html#bertfornextsentenceprediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open(\"../santanlp-corpus/corpus1/train/002.txt\",\"r\").readlines()\n",
    "newsents = [s for s in sentences if not ((len(s.strip()) == 0) or (\"THE END\" in s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 He was buried without mourners, save those detailed for the duty; who, none the less, stiffened in salute of his coffin and called him farewell on the bugle. His death, duly entered in the hospital books, was reported to the Casualty Department; and the Graves Registration clerks took note of his burial and filed it for possible inquiries.\n",
      "\n",
      "507 <BREAK/>\n",
      "\n",
      "508 In the west of Ireland, on the 9th of December, in the town of Ballah, in the Imperial Hotel there was a single guest, clerical and youthful. With the exception of a stray commercial traveller, who stopped once for a night, there had been nobody for a whole month but this guest, and now he was thinking of going away. The town, full enough in summer of trout and salmon fishers, slept all winter like the bears.\n",
      "\n",
      "Prediction: (0 is false:)\n",
      "tensor(0)\n",
      "***\n",
      "890 Sometimes the cotters on the mountains of Donegal hear on windy nights a sudden sound of horses’ hoofs, and say to each other, “There goes Dhoya.” And at the same hour men say if any be abroad in the valleys they see a huge shadow rushing along the mountain.\n",
      "\n",
      "891 <BREAK/>\n",
      "\n",
      "892 'Yet to be loved makes not to love again;\n",
      "\n",
      "Prediction: (0 is false:)\n",
      "tensor(0)\n",
      "***\n",
      "1436 For some years, while her daughter remained an only child, she was passionately devoted to her. But when her son was born she ceased to take much interest in the little girl, who was by this, time rather spoilt, and consequently tiresome. Doll, who proved exemplary in domestic life, took to her when Sibyl forgot her, and became deeply attached to her. Later in life Sibyl became inconsolably jealous of her daughter.\n",
      "\n",
      "1437 <BREAK/>\n",
      "\n",
      "1438 ANTONY HAMMOND told me this story one wet afternoon sitting in the smoking-room of a certain country-house. Everyone else had gone out, regardless of weather, to tramp across the sodden park, walk down to the home farm, or up to the rectory. I observe, when it rains hard some members of a house-party are invariably taken up to tea at the rectory. But neither the plashy grass, nor the manorial pig-styes, nor the clerical teapot seemed to exercise any wild fascination over us; so, with an agreeable conviction of having chosen the better, and dryer, part, we remained at home.\n",
      "\n",
      "Prediction: (0 is false:)\n",
      "tensor(0)\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(newsents)):\n",
    "    sent = newsents[i]\n",
    "    if sent == \"\\n\":\n",
    "        continue\n",
    "    elif \"BREAK\" in sent:\n",
    "        print(i-1, newsents[i-1])\n",
    "        print(i,sent)\n",
    "        print(i+1, newsents[i+1])\n",
    "        inputs_break = tokenizer(newsents[i-1], newsents[i+1], truncation=True,return_tensors='pt')\n",
    "        outputs = model(**inputs_break)\n",
    "        #print(outputs)\n",
    "        print(\"Prediction: (0 is false:)\")\n",
    "        print(torch.argmax(outputs.logits))\n",
    "        print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 William Tully was a little over three-and-twenty when he emerged from the chrysalis stage of his clerkdom and became a Social Reformer. His life and doings until the age of twenty-three, had given small promise of the distinction of his future career; from a mild-mannered, pale-faced and under-sized boy he had developed into a mild-mannered, pale-faced little adult standing five foot five in his boots. Educated at a small private school in the suburbs of London, his record for conduct was practically spotless and he once took a prize for Divinity; further, to the surprise and relief of his preceptors, he managed to scrape through the Senior Cambridge Local Examination before he was transferred to a desk in the office of a London insurance company. His preceptor-in-chief, in a neatly-written certificate, assured his future employers that they would find him painstaking and obedient—and William, for the first six years of his engagement, lived up to the character given him. His mother, a sharp-eyed, masterful woman, had brought him up to be painstaking and obedient; it might be said with truth that as long as she lived he did not know how to be otherwise. It is true he disliked his office superiors vaguely, for the restrictions they placed upon his wishes—just as, for the same reason, he vaguely disliked his mother; but his wishes being indeterminate and his ambition non-existent, his vague dislike never stiffened into active resentment.\n",
      "\n",
      "1 It would seem that the supreme effort of passing his Cambridge Local had left him mentally exhausted for a season; at any rate, from the conclusion of his school-days till he made the acquaintance of Faraday, his reading was practically confined to romantic and humorous literature. He was a regular patron of the fiction department of the municipal lending library and did not disdain to spend modestly on periodicals of the type of Snappy Bits. He was unable to spend more than modestly because his earnings, with the exception of a small sum for fares and pocket-money, were annexed by his mother each Saturday as a matter of normal routine. The manner of her annexation made discussion singularly difficult; and if William ever felt stirrings of rebellion over the weekly cash delivery he was careful never to betray them.\n",
      "\n",
      "Prediction: (0 is correct:)\n",
      "tensor(0)\n",
      "***\n",
      "2 With his colleagues of the office Tully was a negligible quantity. He was not unpopular—it was merely that he did not matter. His mother's control of the family funds was no doubt in part accountable for his comrades' neglect of his society; but his own habits and manners were still more largely to blame, since besides being painstaking and obedient he was unobtrusive and diffident. There was once a project on foot in the office to take him out and make him drunk—but nothing came of it because no one was sufficiently interested in William to give up an evening to the job.\n",
      "\n",
      "3 The crisis in his hitherto well-ordered life came when his mother died suddenly. This was in October 1910. William had gone to the office as usual that morning, leaving his mother apparently in her usual health; he returned in the evening to blinds already drawn down. A neighbour (female) was in waiting in the sitting-room and broke the great news with a sense of its importance and her own; she took William's hand, told him with sniffs that it was the will of the Lord, and entered into clinical details. William sat down rather suddenly when he realized that there would be no one in future to annex his weekly earnings; then, shocked by his lack of filial feeling, he endeavoured to produce an emotion more suited to the solemn occasion. Disconcerted by a want of success which he feared was apparent to his audience, he fidgeted, dry-eyed and awkward—and finally, all things considered, acted well and wisely by demanding to be left alone. To his relief the demand was accepted as reasonable and proper in the first moments of his grief; the sympathizer withdrew, wiping her eyes—unnecessarily—and hoping that God would support him. He locked the door stealthily and stared at his mother's armchair; he was a little afraid of its emptiness, he was also shocked and excited. He knew instinctively that more was to happen, that life from now on would be something new and different.... The arm-chair was empty; the masterful little woman who had borne him, slapped him, managed him and cowed him—the masterful little woman was dead! There was no one now to whom he was accountable; no one of whom he was afraid.... He walked on tiptoe round the tiny room, feeling strangely and pleasantly alive.\n",
      "\n",
      "Prediction: (0 is correct:)\n",
      "tensor(0)\n",
      "***\n",
      "4 The next day increased the sense of his new-found importance; his mother had died rich, as he and she understood riches. She had trusted her son in nothing, not even with the knowledge of her income, and after the stinting and scraping to which she had accustomed him he was amazed to find himself master of a hundred and fifty pounds a year, the interest on capital gradually and carefully invested. In his amazement—at first incredulous—he trod on air, while his mind wandered hazily over the glorious possibilities of opulent years to come; the only alloy in his otherwise supreme content being the necessity for preserving (at least until the funeral was over) a decent appearance of dejection. He felt, too, the need of a friend in whom to confide, some one of his own age and standing before whom it would not be needful to keep up the appearance of dejection and who would not be shocked at the babblings of his stirred and exultant soul; and it was this natural longing for a confidant which, on the day following his mother's funeral, led to the beginning of his friendship with his fellow-clerk, Faraday.\n",
      "\n",
      "5 The head of his department, meeting him in the passage, had said a few perfunctory and conventional words of condolence—whereto William had muttered a sheepish \"Thank you, sir,\" and escaped as soon as might be. The familiar office after his four days' estrangement from it affected him curiously and unpleasantly; he felt his newly-acquired sense of importance slipping gradually away from him, felt himself becoming once again the underling and creature of routine—the William Tully, obedient and painstaking, who had earned from his childhood the favourable contempt of his superiors. It was borne in on him as the hours went by that it was not enough to accept good fortune—good fortune had to be made use of; and he began to make plans in an irregular, tentative fashion, biting the end of his pen and neglecting his work. Should he chuck the office? and if he chucked it, what then? ... Here imagination failed him; his life had been so ordered, so bound down and directed by others, that even his desires were tamed to the wishes of others and left to himself he could not tell what he desired. The need for sympathy and guidance became imperative; driving him, when the other occupants of the room had departed for lunch, to unbosom himself to Faraday.\n",
      "\n",
      "Prediction: (0 is correct:)\n",
      "tensor(0)\n",
      "***\n",
      "6 In his longing to talk he would have addressed himself almost to any one; but on the whole, and in spite of an entire ignorance of his habits and character, he was glad it was Faraday who was left behind to hear him—a newcomer, recently transferred from another branch and, as William realized (if only half-consciously) like himself regarded by their fellow-clerks as a bit of an outsider. A sallow-faced young man, dark-haired and with large hazel eyes, he was neatly garbed as became an insurance clerk; but there was a suggestion of discomfort about his conventional neatness, just as there was a suggestion of effort about his personal cleanliness. He worked hard and steadily; taking no part in the interludes of blithesome chat wherewith his companions enlivened their hours of toil and appearing to be satisfied rather than annoyed by the knowledge of his own isolation. He had spoken to William but two or three times and always in the way of business—nor was his profile bent over a ledger particularly suggestive of sympathy; William's emotions, however, had reached exploding-point, and the door had hardly closed behind the last of their fellows when he blurted out, \"I say,\" and Faraday raised his head.\n",
      "\n",
      "7 \"I say,\" William blurted again, \"did you know—my mother's dead?\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: (0 is correct:)\n",
      "tensor(0)\n",
      "***\n",
      "8 \"Ah—yes,\" said Faraday uncomfortably; he believed he was being appealed to for sympathy, and fidgeted, clearing his throat; \"I—I had heard it mentioned. I needn't say I'm very sorry—extremely.... I suppose you were very much attached to her?\"\n",
      "\n",
      "9 William reflected for a moment and then answered honestly, \"No.\"\n",
      "\n",
      "Prediction: (0 is correct:)\n",
      "tensor(0)\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "batch_first = []\n",
    "batch_second = []\n",
    "for i in range(0,10,2):\n",
    "    sent = newsents[i]\n",
    "    if not \"BREAK\" in sent and len(sent.split(' ')) < 500:\n",
    "        if i+1 in range(len(newsents)):\n",
    "            second_sent = newsents[i+1]\n",
    "            if not \"BREAK\" in second_sent and len(second_sent.split(' ')) < 500:\n",
    "                batch_first.append(sent)\n",
    "                batch_second.append(second_sent)\n",
    "            \n",
    "                print(i,sent)\n",
    "                print(i+1, newsents[i+1])\n",
    "            \n",
    "                inputs_nobreak = tokenizer(sent,newsents[i+1],truncation=True,return_tensors='pt')\n",
    "                outputs = model(**inputs_nobreak)\n",
    "            #print(outputs)\n",
    "                print(\"Prediction: (0 is correct:)\")\n",
    "                print(torch.argmax(outputs.logits))\n",
    "                print(\"***\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.modulesodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pair = tokenizer(newsents[506], newsents[508], truncation=True,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 164])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pair[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2002,  2001,  3950,  2302,  9587, 21737,  2869,  1010,  3828,\n",
       "          2216,  6851,  2005,  1996,  4611,  1025,  2040,  1010,  3904,  1996,\n",
       "          2625,  1010, 16090,  1999, 17664,  1997,  2010, 13123,  1998,  2170,\n",
       "          2032, 13407,  2006,  1996, 11829,  2571,  1012,  2010,  2331,  1010,\n",
       "         25073,  3133,  1999,  1996,  2902,  2808,  1010,  2001,  2988,  2000,\n",
       "          1996, 19844,  2533,  1025,  1998,  1996,  9729,  8819, 26706,  2165,\n",
       "          3602,  1997,  2010,  8940,  1998,  6406,  2009,  2005,  2825, 27050,\n",
       "          1012,   102,  1999,  1996,  2225,  1997,  3163,  1010,  2006,  1996,\n",
       "          6280,  1997,  2285,  1010,  1999,  1996,  2237,  1997,  3608,  4430,\n",
       "          1010,  1999,  1996,  4461,  3309,  2045,  2001,  1037,  2309,  4113,\n",
       "          1010, 23106,  1998, 22446,  1012,  2007,  1996,  6453,  1997,  1037,\n",
       "         15926,  3293, 21916,  1010,  2040,  3030,  2320,  2005,  1037,  2305,\n",
       "          1010,  2045,  2018,  2042,  6343,  2005,  1037,  2878,  3204,  2021,\n",
       "          2023,  4113,  1010,  1998,  2085,  2002,  2001,  3241,  1997,  2183,\n",
       "          2185,  1012,  1996,  2237,  1010,  2440,  2438,  1999,  2621,  1997,\n",
       "         13452,  1998, 11840,  8731,  2015,  1010,  7771,  2035,  3467,  2066,\n",
       "          1996,  6468,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 164])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pair['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok_pair['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsents[508].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 72])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_first = tokenizer(newsents[506], truncation=True,return_tensors='pt')\n",
    "tok_first['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1999,\n",
       " 1996,\n",
       " 2225,\n",
       " 1997,\n",
       " 3163,\n",
       " 1010,\n",
       " 2006,\n",
       " 1996,\n",
       " 6280,\n",
       " 1997,\n",
       " 2285,\n",
       " 1010,\n",
       " 1999,\n",
       " 1996,\n",
       " 2237,\n",
       " 1997,\n",
       " 3608,\n",
       " 4430,\n",
       " 1010,\n",
       " 1999,\n",
       " 1996,\n",
       " 4461,\n",
       " 3309,\n",
       " 2045,\n",
       " 2001,\n",
       " 1037,\n",
       " 2309,\n",
       " 4113,\n",
       " 1010,\n",
       " 23106,\n",
       " 1998,\n",
       " 22446,\n",
       " 1012,\n",
       " 2007,\n",
       " 1996,\n",
       " 6453,\n",
       " 1997,\n",
       " 1037,\n",
       " 15926,\n",
       " 3293,\n",
       " 21916,\n",
       " 1010,\n",
       " 2040,\n",
       " 3030,\n",
       " 2320,\n",
       " 2005,\n",
       " 1037,\n",
       " 2305,\n",
       " 1010,\n",
       " 2045,\n",
       " 2018,\n",
       " 2042,\n",
       " 6343,\n",
       " 2005,\n",
       " 1037,\n",
       " 2878,\n",
       " 3204,\n",
       " 2021,\n",
       " 2023,\n",
       " 4113,\n",
       " 1010,\n",
       " 1998,\n",
       " 2085,\n",
       " 2002,\n",
       " 2001,\n",
       " 3241,\n",
       " 1997,\n",
       " 2183,\n",
       " 2185,\n",
       " 1012,\n",
       " 1996,\n",
       " 2237,\n",
       " 1010,\n",
       " 2440,\n",
       " 2438,\n",
       " 1999,\n",
       " 2621,\n",
       " 1997,\n",
       " 13452,\n",
       " 1998,\n",
       " 11840,\n",
       " 8731,\n",
       " 2015,\n",
       " 1010,\n",
       " 7771,\n",
       " 2035,\n",
       " 3467,\n",
       " 2066,\n",
       " 1996,\n",
       " 6468,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_second = tokenizer(newsents[508], truncation=True)#,return_tensors='pt')\n",
    "tok_second['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] in the west of ireland, on the 9th of december, in the town of ballah, in the imperial hotel there was a single guest, clerical and youthful. with the exception of a stray commercial traveller, who stopped once for a night, there had been nobody for a whole month but this guest, and now he was thinking of going away. the town, full enough in summer of trout and salmon fishers, slept all winter like the bears.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tok_second['input_ids'][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] he was buried without mourners, save those detailed for the duty ; who, none the less, stiffened in salute of his coffin and called him farewell on the bugle. his death, duly entered in the hospital books, was reported to the casualty department ; and the graves registration clerks took note of his burial and filed it for possible inquiries. [SEP] in the west of ireland, on the 9th of december, in the town of ballah, in the imperial hotel there was a single guest, clerical and youthful. with the exception of a stray commercial traveller, who stopped once for a night, there had been nobody for a whole month but this guest, and now he was thinking of going away. the town, full enough in summer of trout and salmon fishers, slept all winter like the bears. [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tok_pair = tokenizer(newsents[506], newsents[508], truncation=True)\n",
    "tokenizer.decode(tok_pair['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'was',\n",
       " 'buried',\n",
       " 'without',\n",
       " 'mo',\n",
       " '##urne',\n",
       " '##rs',\n",
       " ',',\n",
       " 'save',\n",
       " 'those',\n",
       " 'detailed',\n",
       " 'for',\n",
       " 'the',\n",
       " 'duty',\n",
       " ';',\n",
       " 'who',\n",
       " ',',\n",
       " 'none',\n",
       " 'the',\n",
       " 'less',\n",
       " ',',\n",
       " 'stiffened',\n",
       " 'in',\n",
       " 'salute',\n",
       " 'of',\n",
       " 'his',\n",
       " 'coffin',\n",
       " 'and',\n",
       " 'called',\n",
       " 'him',\n",
       " 'farewell',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bug',\n",
       " '##le',\n",
       " '.',\n",
       " 'his',\n",
       " 'death',\n",
       " ',',\n",
       " 'duly',\n",
       " 'entered',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hospital',\n",
       " 'books',\n",
       " ',',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'to',\n",
       " 'the',\n",
       " 'casualty',\n",
       " 'department',\n",
       " ';',\n",
       " 'and',\n",
       " 'the',\n",
       " 'graves',\n",
       " 'registration',\n",
       " 'clerks',\n",
       " 'took',\n",
       " 'note',\n",
       " 'of',\n",
       " 'his',\n",
       " 'burial',\n",
       " 'and',\n",
       " 'filed',\n",
       " 'it',\n",
       " 'for',\n",
       " 'possible',\n",
       " 'inquiries',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(newsents[506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2002,\n",
       " 2001,\n",
       " 3950,\n",
       " 2302,\n",
       " 9587,\n",
       " 21737,\n",
       " 2869,\n",
       " 1010,\n",
       " 3828,\n",
       " 2216,\n",
       " 6851,\n",
       " 2005,\n",
       " 1996,\n",
       " 4611,\n",
       " 1025,\n",
       " 2040,\n",
       " 1010,\n",
       " 3904,\n",
       " 1996,\n",
       " 2625,\n",
       " 1010,\n",
       " 16090,\n",
       " 1999,\n",
       " 17664,\n",
       " 1997,\n",
       " 2010,\n",
       " 13123,\n",
       " 1998,\n",
       " 2170,\n",
       " 2032,\n",
       " 13407,\n",
       " 2006,\n",
       " 1996,\n",
       " 11829,\n",
       " 2571,\n",
       " 1012,\n",
       " 2010,\n",
       " 2331,\n",
       " 1010,\n",
       " 25073,\n",
       " 3133,\n",
       " 1999,\n",
       " 1996,\n",
       " 2902,\n",
       " 2808,\n",
       " 1010,\n",
       " 2001,\n",
       " 2988,\n",
       " 2000,\n",
       " 1996,\n",
       " 19844,\n",
       " 2533,\n",
       " 1025,\n",
       " 1998,\n",
       " 1996,\n",
       " 9729,\n",
       " 8819,\n",
       " 26706,\n",
       " 2165,\n",
       " 3602,\n",
       " 1997,\n",
       " 2010,\n",
       " 8940,\n",
       " 1998,\n",
       " 6406,\n",
       " 2009,\n",
       " 2005,\n",
       " 2825,\n",
       " 27050,\n",
       " 1012]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(newsents[506]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
